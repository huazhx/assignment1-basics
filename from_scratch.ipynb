{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6fac222",
   "metadata": {},
   "source": [
    "## BPETokenzier \n",
    "\n",
    "### components\n",
    "\n",
    "- vocab_init\n",
    "  - special_tokens\n",
    "\n",
    "- pre_tokenization(multiprocess)\n",
    "  - pre_tokenize_worker\n",
    "  - get_frequency_table {(l,o,w): 5 ...}\n",
    "\n",
    "- merge\n",
    "  - get_successive_pairs {lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}.\n",
    "  - get_lexicographically_greater_pair\n",
    "  - merge_pre_tokens {(l,o,w): 5, (l,o,w,e,r): 2, (w,i,d,e,st): 3, (n,e,w,e,st): 6}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9252ae",
   "metadata": {},
   "source": [
    "- Level 0\n",
    "  - tokenize_word: word -> char list\n",
    "  - count_pairs: 统计token列表中相邻对\n",
    "  - merge_pair_in_word: 合并指定的pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec590ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(word: str) -> list[str]: \n",
    "    assert word\n",
    "    return [char for char in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f929e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_word 测试通过！\n"
     ]
    }
   ],
   "source": [
    "# 简单测试\n",
    "assert tokenize_word(\"hello\") == ['h', 'e', 'l', 'l', 'o']\n",
    "assert tokenize_word(\"a\") == ['a']\n",
    "print(\"tokenize_word 测试通过！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c89dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_pairs(token_list: list[str]) -> dict[tuple[str, str], int]:\n",
    "    pair_freq = defaultdict(int)\n",
    "    for i in range(0, len(token_list) - 1):\n",
    "        token_pair = (token_list[i], token_list[i+1])\n",
    "        pair_freq[token_pair] += 1\n",
    "    \n",
    "    return pair_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01903824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "assert count_pairs(['h', 'e', 'l', 'l', 'o']) == {('h','e'): 1, ('e','l'): 1, ('l','l'): 1, ('l','o'): 1}\n",
    "print('Pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db87f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair_in_word(word: list[str], pair: tuple[str, str]):\n",
    "    new_word = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
    "            new_word.append(word[i] + word[i+1])\n",
    "            i += 2 \n",
    "        else:\n",
    "            new_word.append(word[i])\n",
    "            i += 1\n",
    "    return new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a57a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试1：基础合并\n",
    "assert merge_pair_in_word(['h', 'e', 'l', 'l', 'o'], ('l', 'l')) == ['h', 'e', 'll', 'o']\n",
    "\n",
    "# 测试2：多次出现\n",
    "assert merge_pair_in_word(['a', 'b', 'a', 'b', 'c'], ('a', 'b')) == ['ab', 'ab', 'c']\n",
    "\n",
    "# 测试3：没有匹配\n",
    "assert merge_pair_in_word(['h', 'e', 'l', 'o'], ('l', 'l')) == ['h', 'e', 'l', 'o']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748858ca",
   "metadata": {},
   "source": [
    "### Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "936def32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('l', 'o'), 2),\n",
       " (('l', 'l'), 2),\n",
       " (('h', 'e'), 2),\n",
       " (('e', 'l'), 2),\n",
       " (('w', 'o'), 1),\n",
       " (('r', 'l'), 1),\n",
       " (('o', 'r'), 1),\n",
       " (('l', 'd'), 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 维护tokenized_word\n",
    "tokenized_word_freq = dict()\n",
    "\n",
    "# 模拟数据\n",
    "words = [\"hello\", \"world\", \"hello\"]\n",
    "word_freq = defaultdict(int)\n",
    "for word in words:\n",
    "    word_freq[word] += 1\n",
    "\n",
    "tokenized_word_freq_d = defaultdict(int)\n",
    "for word, count in word_freq.items():\n",
    "    tokenized_word = tokenize_word(word)\n",
    "    tokenized_word_freq_d[tuple(tokenized_word)] = count\n",
    "\n",
    "tokenized_word_freq = dict(tokenized_word_freq_d)\n",
    "\n",
    "pair_freq = defaultdict(int)\n",
    "for tokenized_word, count in tokenized_word_freq.items():\n",
    "    part_pair_freq =  count_pairs(tokenized_word)\n",
    "    for pf, freq in part_pair_freq.items():\n",
    "        pair_freq[pf] += freq * count\n",
    "\n",
    "sorted(dict(pair_freq).items(), key = lambda x: (x[1], x[0][0], x[0][1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c2e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
